{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f019c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb48059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Jadha\\PycharmProjects\\python-stuff\\data_science_job_data_with_applylink_frontend.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f287e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"The process of searching for a job can be very stressful, but it doesn’t have to be. Start with a\\\n",
    "        well-written resume that has appropriate keywords for your occupation. Next, conduct a targeted job search\\\n",
    "        for positions that meet your needs.\",\n",
    "        \"Gardening in mixed beds is a great way to get the most productivity from a small space. Some investment\\\n",
    "        is required, to purchase materials for the beds themselves, as well as soil and compost. The\\\n",
    "        investment will likely pay-off in terms of increased productivity.\",\n",
    "        \"Looking for a job can be very stressful, but it doesn’t have to be. Begin by writing a good resume with\\\n",
    "        appropriate keywords for your occupation. Second, target your job search for positions that match your\\\n",
    "        needs.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22a2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057c1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=30, min_count=2, epochs=80)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf0acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee67f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52462b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"d2v.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7575ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2', 0.998757541179657)\n"
     ]
    }
   ],
   "source": [
    "similar_doc = model.dv.most_similar('0')\n",
    "print(similar_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5b618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [-0.2233547   0.19589196  0.20418659  0.04797274 -0.05891578 -0.02845262\n",
      " -0.11194982 -0.02517237 -0.69699144 -0.09624445  0.13146943 -0.05101014\n",
      " -0.17438155 -0.635835   -0.05463094 -0.35788196  0.15726508 -0.03164224\n",
      " -0.2858524  -0.20564233  0.22807753  0.06858576 -0.2593847   0.16441761\n",
      "  0.4163114   0.13707867  0.13686807 -0.01985187  0.05317551 -0.34940723]\n"
     ]
    }
   ],
   "source": [
    "test_data = word_tokenize(\"When your focus is to improve employee performance, it’s essential to encourage ongoing\\\n",
    "                        dialogue between managers and their direct reports. Some companies encourage supervisors\\\n",
    "                        to hold one-on-one meetings with employees as a way to facilitate\\\n",
    "                        two-way communication.\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8775507",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cosine_similarities() missing 2 required positional arguments: 'vector_1' and 'vectors_all'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcosine_similarities\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: cosine_similarities() missing 2 required positional arguments: 'vector_1' and 'vectors_all'"
     ]
    }
   ],
   "source": [
    "model.dv.cosine_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341470fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from num2words import num2words\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_words_to_norm_words_func(text):\n",
    "    '''\n",
    "    Replaces common chat expressions with their spelled out form\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string with replaced chat expressions\n",
    "    ''' \n",
    "    return re.sub(r'\\S+', lambda m: chat_expressions_dict.get(m.group().upper(), m.group()) , text)\n",
    "\n",
    "def sep_num_words_func(text):\n",
    "    '''\n",
    "    Separates numbers from words or other characters\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string with separated numbers from words or other characters\n",
    "    ''' \n",
    "    return re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", text).strip() \n",
    "\n",
    "def num_to_words(text):\n",
    "    '''\n",
    "    Convert Numbers to Words\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string with converted Numbers to Words\n",
    "    ''' \n",
    "    after_spliting = text.split()\n",
    "\n",
    "    for index in range(len(after_spliting)):\n",
    "        if after_spliting[index].isdigit():\n",
    "            after_spliting[index] = num2words(after_spliting[index])\n",
    "    numbers_to_words = ' '.join(after_spliting)\n",
    "    return numbers_to_words\n",
    "\n",
    "\n",
    "def convert_exp_to_word(text):\n",
    "    replacement = {'8+': 'eight plus', '9+': 'nine plus', '10+': 'ten plus', '11+': 'eleven plus', '12+': 'twelve plus', '13+': 'thirteen plus', '14+': 'fourteen plus', '15+': 'fifteen plus', '16+': 'sixteen plus', '17+': 'seventeen plus', '18+': 'eighteen plus', '19+': 'nineteen plus', '20+': 'twenty plus'}\n",
    "\n",
    "    for pat, rep in replacement.items():\n",
    "        text = re.sub(pat, rep, text)\n",
    "    return text\n",
    "\n",
    "def remove_extra_whitespaces_func(text):\n",
    "    '''\n",
    "    Removes extra whitespaces from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without extra whitespaces\n",
    "    ''' \n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "def stop_word_removal(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # converts the words in word_tokens to lower case and then checks whether \n",
    "    #they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    #with no lower case conversion\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "def num_to_words(text):\n",
    "    '''\n",
    "    Convert Numbers to Words\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string with converted Numbers to Words\n",
    "    ''' \n",
    "    after_spliting = text.split()\n",
    "\n",
    "    for index in range(len(after_spliting)):\n",
    "        if after_spliting[index].isdigit():\n",
    "            after_spliting[index] = num2words(after_spliting[index])\n",
    "    numbers_to_words = ' '.join(after_spliting)\n",
    "    return numbers_to_words\n",
    "\n",
    "def remove_punctuation_func(text):\n",
    "    '''\n",
    "    Removes all punctuation from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without punctuations\n",
    "    '''\n",
    "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('stsb-distilroberta-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392683c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli delete-cache\n",
    "# !pip install huggingface_hub[cli]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I like Python because I can build AI applications\"\n",
    "sentence2 = \"I like Python because I can do data analytics\"\n",
    "# encode sentences to get their embeddings\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "# compute similarity scores of two embeddings\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Sentence 1:\", sentence1)\n",
    "print(\"Sentence 2:\", sentence2)\n",
    "print(\"Similarity score:\", cosine_scores.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88226984",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_string = '''\n",
    "\n",
    "Skills Summary\n",
    "• CS Fundamentals: Data Structures and Algorithms, Time Complexity Analysis, Database Management System,\n",
    "OOPs (Object Oriented Programming), Operating System\n",
    "• Languages: C++, Javascript, Bash, Python, PHP, C++, JavaScript, SQL\n",
    "• Frameworks: Django, ReactJS, NodeJS, XAMPP\n",
    "• Tools: MySQL, Docker,NGINX, SQLite, Oracle SQL(Apex),git, C8051F120 Microcontroller\n",
    "• Platforms: Azure, Linux, AWS\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = stop_word_removal(remove_extra_whitespaces_func(stop_word_removal(df['description'][2]))).lower()\n",
    "two = stop_word_removal(remove_extra_whitespaces_func(stop_word_removal(resume_string))).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498af331",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = model.encode(df['description'][2], convert_to_tensor=True)\n",
    "two = model.encode(resume_string, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = util.pytorch_cos_sim(one, two)\n",
    "# print(\"Sentence 1:\", df['description'][1])\n",
    "# print(\"Sentence 2:\", resume_string)\n",
    "print(\"Similarity score:\", cosine_scores.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4aa14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['description'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resume_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['description'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fae65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
